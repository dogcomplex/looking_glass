>AFAIK getting under 1E-16J/bit for 2 way conversion without doing anything fundamental (R&D project unto itself) would be overly optimistic.  That sets a floor to the J/FLOP & J/tk which will be better than H100s but my guess is not by 100x.
5:46 p.m.


response?

---


Great pushback. Hereâ€™s a tight, numbers-aware reply you can send that doesnâ€™t hand-wave the conversion floor, but also shows why 100Ã— OPEX is still plausible in the right regime:

Totally agree that E/O + O/E conversion energy matters. Two clarifications:

We donâ€™t pay conversion per-MAC in an optical VMM/loop.
You convert an N-vector once (Eâ†’O), let the optics do NÃ—M MACs â€œfor free,â€ then read M outputs once (Oâ†’E). So even if the two-way conversion has a hard floor, itâ€™s amortized over millions of MACs per pass. Thatâ€™s the key difference from digital where every MAC drags bits across SRAM/HBM.

Shot noise, not just electronics, sets the optical floor.
At 1550 nm, one photon â‰ˆ 1.28Ã—10â»Â¹â¹ J. If you target ~10Â³ photons per measurement for decent SNR, thatâ€™s â‰ˆ 1.3Ã—10â»Â¹â¶ J per output sample, i.e., the same ballpark as your 10â»Â¹â¶ J/bit assertion. Again: amortized across M outputs and N inputs.

Back-of-envelope to show the scaling:

Let E_in be the energy to encode the N inputs optically (DACs/modulators, driver losses).

Let E_out be the energy to read M outputs (detector + TIA + ADC).

One optical pass does NÃ—M MACs.

Energy per MAC â‰ˆ (E_in + E_out)/(NÃ—M) + E_optics/MAC.

If youâ€™re in a moderately wide regime (say N=M=1024), even pessimistic per-pass numbers like E_in + E_out â‰ˆ tens of nJ land you at â‰² tens of fJ/MAC. By comparison, an H100 at ~700 W delivering ~1.5 PFLOP/s FP16 is ~0.5 pJ/FLOP (â‰ˆ 500 fJ)â€”and real LLM tokens burn far more due to memory traffic. So a 10â€“100Ã— energy win is not fantasy when:

You keep the loop analog (no per-layer digitize/rediscretize).

You batch large NÃ—M (big amortization).

You co-locate/encode weights optically (MRRs/PCM/DMD/SLM) to avoid per-token DAC overhead.

You avoid camera-class readouts for the fast path (use PD arrays / WDM taps).

Where the 100Ã— falls apart:

Small N or M (poor amortization).

Frequent A/Dâ†”D/A inside the loop (you pay the floor every layer).

Weighting driven by high-precision DACs every token instead of persistent optical states.

Poor SNR forcing high optical power or multiple reads.

Two practical counters to the â€œconversion floor caps gainsâ€ argument:

Architectural avoidance: Do E/O once per token (or per K tokens) and O/E once per layer-group (or only at the very end). That collapses conversion energy per token dramatically.

Parallelism makes the floor tiny per MAC: Even if your 2-way floor is 10â»Â¹â´â€“10â»Â¹â¶ J/bit, spreading it across 10â¶â€“10â¸ MACs per pass yields sub-10 fJ/MACâ€”comfortably 50â€“100Ã— below the effective energy/â€œuseful MACâ€ in GPU inference once memory is included.

Finally, note why GPUs look worse in practice: the energy you quote as â€œJ/FLOPâ€ hides HBM traffic (pJ/bit) and on-chip data motion. Optical loops eliminate most of that; their routing cost is largely distance-agnostic and the â€œcomputeâ€ happens as light propagates.

Bottom line:

If you digitize every layer: I agreeâ€”no 100Ã—.

If you keep the inner stack analog, persist weights optically, and amortize conversions over wide VMMs and many Î» (WDM), 10â€“100Ã— OPEX versus GPUs is very realistic for the right workloads. The lever isnâ€™t magic converters; itâ€™s reducing how often you need them and how many MACs they amortize over.

Happy to run a quick param sweep for your simulator (pick N, M, Î» count, detector SNR, ADC bits, modulator drive) and plot E/MAC and J/token under different â€œconversion floorâ€ assumptions.

---



Full context here: 
initial post:

to add an option to people's radars, getting 10-100x better energy efficiency on transformer inference and training seems quite plausible with off the shelf 1550nm fiberoptics hardware

am building sims.  may buy and build prototype if they continue to work.  am perpetually very tired though

would be a very nice disruptor to datacenters though and the tech is already too widely distributed to be bottlenecked/monopolized

ideally package as a consumer appliance and decentralize network RL train with them, scale well past legacy models for pennies on the dollar.  kickstarter/shitcoin it to crowdfund

INTELLECT-2: The First Globally Distributed Reinforcement Learning Training of a 32B Parameter Model
Today we are launching INTELLECT-2: the first 32B parameter globally decentralized Reinforcement Learning training run where anyone can permissionlessly contribute their heterogeneous compute resources.
www.primeintellect.ai

https://www.primeintellect.ai/blog/intellect-2

(bonus: would give lightning-fast response times locally which cloud can never match.  pair with robots for responsiveness overkill)

AOC - Microsoft Research
A new kind of computer â€“ an analog optical computer â€“ that has the potential to accelerate AI inference and hard optimization workloads by 100x, leveraging hardware-software co-design to improve the efficiency and sustainability of real-world applications.
www.microsoft.com
Nov 7, 2025

https://www.microsoft.com/en-us/research/project/aoc/

microsoft did prototype already.  would basically do the same but just fiberoptics and more conservative, but tileable scaling




>>response:
>It's the exponential & normalization that's harder to do in the optical domain.   Not to say I don't have thoughts on that, just that a 1:1 mapping of transformer training to all-optical is nontrivial.

my reply;
yep, which is why definitely aim for it to be an optical co-processor, with a (cheap) gpu doing the normalization stitching.  offloads 90-99% of the work to optics.  though I think mapping to ternary bits with a saturable absorber + amplifier would allow keeping it in an analog loop fairly reliably/cheaply - not needed, but another 10x speedup to avoid digital conversion through multiple layers.   similar add-ons and yet another 10x speedup if we can do memory read/write onto phased glass - then never have to touch digital til compute is done.  but much more ambitious.  pull that all off tho and it's a desktop datacenter...

>>their reply:
>AFAIK getting under 1E-16J/bit for 2 way conversion without doing anything fundamental (R&D project unto itself) would be overly optimistic.  That sets a floor to the J/FLOP & J/tk which will be better than H100s but my guess is not by 100x.


I think I might have miscontrued initial 10-100x claims as being without the analog loop, which is fair critique if so.  What would be your claims for each part probably?  single-pass no loop (we could batch though for speedups).  then add the analog loop.  then ambitious persistence of analog memory etc.   I thought 10-100x range was doable even with single pass, and only gets better with the additionals, but could be overly ambitious





---- 


Hereâ€™s a clean reply you can paste back, then Iâ€™ll give you tier-by-tier numbers and what your simulator should sweep.

Pasteable reply (tight + numbers-aware)

Totally fairâ€”there is a hard floor for two-way E/Oâ†”O/E conversion. The key is we donâ€™t pay it per-MAC in an optical VMM/loop.

One optical pass does 
ğ‘
Ã—
ğ‘€
NÃ—M MACs â€œfor freeâ€ in flight. You convert the 
ğ‘
N-vector once (Eâ†’O), do all 
ğ‘
Ã—
ğ‘€
NÃ—M multiplies/adds in optics, then read 
ğ‘€
M numbers (Oâ†’E) once. So the conversion floor is amortized:

ğ¸
MAC
â‰ˆ
ğ¸
in
+
ğ¸
out
ğ‘
â€‰
ğ‘€
+
ğ¸
optics/MAC
.
E
MAC
	â€‹

â‰ˆ
NM
E
in
	â€‹

+E
out
	â€‹

	â€‹

+E
optics/MAC
	â€‹

.

At 1550 nm, 1 photon â‰ˆ 
1.28
â€‰â£
Ã—
â€‰â£
10
âˆ’
19
1.28Ã—10
âˆ’19
 J. Targeting ~
10
3
10
3
 photons/sample for decent SNR is 
âˆ¼
10
âˆ’
16
âˆ¼10
âˆ’16
 J per output sampleâ€”same ballpark as your 
10
âˆ’
16
10
âˆ’16
 J/bit pointâ€”but thatâ€™s per output, not per-MAC. With modest width (say 
ğ‘
=
ğ‘€
=
1024
N=M=1024), even pessimistic 
ğ¸
in
+
ğ¸
out
âˆ¼
10
â€‰â£
âˆ’
â€‰â£
100
E
in
	â€‹

+E
out
	â€‹

âˆ¼10âˆ’100 nJ per pass yields 
âˆ¼
10
â€‰â£
âˆ’
â€‰â£
100
âˆ¼10âˆ’100 fJ/MAC; with a 12-layer analog loop the conversion term drops another 12Ã— to <1â€“10 fJ/MAC. For comparison, an H100-class GPU is 
ğ‘‚
(
10
2
â€‰â£
âˆ’
â€‰â£
10
3
)
O(10
2
âˆ’10
3
) fJ per useful MAC once HBM traffic is included, so 10â€“100Ã— OPEX is realistic when:

you keep inner layers analog (no per-layer digitize),

batch wide 
ğ‘
,
ğ‘€
N,M (strong amortization),

persist weights optically (no per-token DAC drive),

and avoid camera-style readouts (use PD arrays/WDM).

Where it isnâ€™t 100Ã—: small 
ğ‘
N/or 
ğ‘€
M; frequent A/Dâ†”D/A inside the loop; high-precision per-token DACing of weights; low SNR forcing high optical power or repeats.

Happy to run a quick param sweep: pick 
ğ‘
,
ğ‘€
,
ğ¿
,
ğœ†
N,M,L,Î», detector SNR, ADC bits, modulator drive; weâ€™ll plot fJ/MAC and J/token under different â€œconversion floorâ€ assumptions.

Whatâ€™s realistic at each build tier?

Assume ballpark, conservative device-level design (C-band, off-the-shelf fiber parts). These are order-of-magnitude targets to steer design/simâ€”not promises.

Baseline to compare against (for useful MACs)

Modern GPU inference (HBM in the loop): effective 200â€“2000 fJ/MAC depending on model/memory traffic; J/token varies wildly by context length, KV reuse, precision, etc.

Time/latency: GPUs excel at short/skinny matmuls; optics excels as width grows and when you can keep a loop analog.

Tier A â€” Single-pass VMM, no loop (cheap de-risk)

Flow: Eâ†’O once (encode 
ğ‘
N inputs), free-space/fiber multiply-accumulate, Oâ†’E once (read 
ğ‘€
M outputs). Digital does the rest.

Energy model: 
ğ¸
MAC
â‰ˆ
(
ğ¸
in
+
ğ¸
out
)
/
(
ğ‘
ğ‘€
)
+
ğ¸
photons
/
(
ğ‘
ğ‘€
)
E
MAC
	â€‹

â‰ˆ(E
in
	â€‹

+E
out
	â€‹

)/(NM)+E
photons
	â€‹

/(NM).

Pessimistic 
ğ¸
in
+
ğ¸
out
E
in
	â€‹

+E
out
	â€‹

: 10â€“100 nJ per pass (aggregated drivers + TIAs/ADCs).

Example 
ğ‘
=
ğ‘€
=
1024
N=M=1024: ~10â€“100 fJ/MAC.

Throughput: gated by modulators + PD/ADC bandwidth; multi-Î» (WDM) gives linear speedup.

Latency: one optical flight + I/O; decent for batched matmuls; not ideal for layer-by-layer token loops.

Relative win: ~3â€“20Ã— energy vs GPUs when 
ğ‘
,
ğ‘€
â‰³
1
ğ‘˜
N,Mâ‰³1k and you amortize I/O; <3Ã— if itâ€™s narrow/small.

Use: de-risk physics + plumbing; great for offloading wide attention/VMMs in a hybrid stack.

Tier B â€” Add analog loop for L layers (your target)

Flow: Eâ†’O once per token (or block), L optical layer steps in a loop (SOA gain + saturable nonlinearity), Oâ†’E once.

Energy model:

ğ¸
MAC
â‰ˆ
ğ¸
in
+
ğ¸
out
ğ¿
â€‰
ğ‘
ğ‘€
+
ğ¸
loop/MAC
.
E
MAC
	â€‹

â‰ˆ
LNM
E
in
	â€‹

+E
out
	â€‹

	â€‹

+E
loop/MAC
	â€‹

.

Conversion amortizes by 
ğ¿
L; loop adds SOA + nonlinearity photons.

Typical: 
ğ¿
=
8
â€‰â£
âˆ’
â€‰â£
16
L=8âˆ’16, 
ğ‘
,
ğ‘€
=
1
â€‰â£
âˆ’
â€‰â£
4
â€‰
k
N,M=1âˆ’4k, 
ğ¸
in
+
ğ¸
out
=
20
â€‰â£
âˆ’
â€‰â£
100
E
in
	â€‹

+E
out
	â€‹

=20âˆ’100 nJ â‡’ conversion term <1â€“10 fJ/MAC; add loop term ~1â€“10 fJ/MAC â‡’ ~2â€“20 fJ/MAC total.

Relative win: ~10â€“100Ã— energy vs GPU on the inner stack; big throughput/latency gains if you keep the loop hot.

Use: optical co-processor does inner transformer blocks; GPU stitches norms/softmax/residuals.

Tier C â€” Persist weights optically (DMD/SLM/PCM/MRR; reprogram infrequently)

Flow: weights live in optics; no per-token DAC to â€œwriteâ€ them.

Energy effect: knocks down 
ğ¸
in
E
in
	â€‹

 substantially; reprogram cost amortized over many tokens/batches.

Typical: effective <1â€“10 fJ/MAC in-loop for well-amortized workloads (big 
ğ‘
,
ğ‘€
,
ğ¿
,
ğœ†
N,M,L,Î»); can push toward the single-digit fJ/MAC regime.

Use: steady models/adapter layers; sparse/LoRA or attention heads that donâ€™t change per token.

Tier D â€” Analog state/memory (ambitious) (e.g., holographic/glass or resonant stores)

Flow: keep K/V, activations, or grads in analog form across steps, avoiding digitize-store-recall.

Energy effect: removes a lot of HBM-like traffic; can push overall J/token >100Ã— below digital on memory-dominated paths.

Risk: stability, drift, addressing, write endurance, calibration.

Use: later roadmap; not needed to show strong wins on Tier B/C.

Where the 100Ã— doesnâ€™t show up

ğ‘
N or 
ğ‘€
M small (poor amortization).

Per-layer A/Dâ†”D/A (you pay the floor each layer).

High-precision per-token weight DACs instead of persistent optical weights.

Low SNR (forces optical power up or repeats).

Camera-style readouts in the fast path (prefer PD arrays/WDM taps).

What to add to your simulator (so the â€œfloorâ€ bites properly)

Model these knobs explicitly and sweep them:

E/O & O/E budgets

ğ¸
in
E
in
	â€‹

, 
ğ¸
out
E
out
	â€‹

: sweep 5â€“200 nJ per pass (aggregate) to cover drivers, TIAs/ADCs, control logic.

ADC bits: 6â€“10 b; higher bits increase 
ğ¸
out
E
out
	â€‹

.

Modulator/PD bandwidth limits (sets cycle time).

Photon/SNR physics

Photon energy 
â„
ğœˆ
hÎ½ at 1550 nm; per-sample photon budget (e.g., 300â€“3000).

Shot noise + TIA noise â†’ required optical power.

Saturable absorber/soft-nonlinearity curve (ternary: 
âˆ’
1
,
0
,
+
1
âˆ’1,0,+1 bins) and its energy.

Geometry & amortization

ğ‘
,
ğ‘€
âˆˆ
[
256
,
4096
]
N,Mâˆˆ[256,4096]; Î» count (WDM: 1â€“16); layers 
ğ¿
L in loop (1â€“24).

Compute:

ğ¸
MAC
=
ğ¸
in
+
ğ¸
out
ğ‘
ğ‘€
ğ¿
+
ğ¸
loop/MAC
(
photons
,
gain
)
.
E
MAC
	â€‹

=
NML
E
in
	â€‹

+E
out
	â€‹

	â€‹

+E
loop/MAC
	â€‹

(photons,gain).

Throughput: samples/sec = min(modulator rate, PD/ADC rate) Ã— Î».

Latency: time-to-first-token vs time-per-token (loop dwell + readout).

Weight programming model

Tier A/B: per-token DAC drive cost (include or zero it).

Tier C: one-time reprogram (e.g., mJâ€“J), amortized over tokens per program. Plot sensitivity.

Stability

Loop gain window; drift; calibration overhead (insert periodic O/E for AGC and account its energy/time).

Output plots youâ€™ll care about:

fJ/MAC vs. (N,M,L,Î») under several 
ğ¸
in
+
ğ¸
out
E
in
	â€‹

+E
out
	â€‹

 floors.

J/token for a target transformer block shape (e.g., 2â€“4 k width) vs GPU â€œeffectiveâ€ ranges.

Latency (TTFT and tokens/s) vs Î» and ADC rate.

Sensitivity to SNR (photons/sample) and ADC bits on accuracy.

Calibrating your public claims

Single-pass (no loop): â€œ3â€“20Ã— energyâ€ on wide VMMs; â€œsimilar or better latencyâ€ at large batch/width; not 100Ã— except at very large 
ğ‘
,
ğ‘€
N,M with multi-Î».

Add analog loop: â€œ10â€“100Ã— energyâ€ on inner layers with 
ğ¿
â‰³
8
Lâ‰³8, 
ğ‘
,
ğ‘€
â‰³
1
ğ‘˜
N,Mâ‰³1k, and PD/WDM readouts; large throughput gains.

Persist weights optically: solidifies the upper end of that 10â€“100Ã— for steady models/blocks.

Analog memory: future >100Ã— on memory-bound paths; R&D.

For CAPEX narratives: itâ€™s reasonable to say â€œoff-the-shelf C-band parts can build a co-processor that beats a single high-end GPU on the right wide workloads for similar or lower capex; racks of tiles can rival small GPU clustersâ€â€”and keep precise $ claims tied to a concrete BOM once your simulator locks in 
ğ‘
,
ğ‘€
,
ğ¿
,
ğœ†
N,M,L,Î» and readout choice.

If you want, I can turn the formulas above into a tiny sheet with sliders for 
ğ‘
,
ğ‘€
,
ğ¿
,
ğœ†
N,M,L,Î», photons/sample, and 
ğ¸
in
+
ğ¸
out
E
in
	â€‹

+E
out
	â€‹

, so you can snapshot scenarios (single-pass vs loop vs persistent weights) and quote fJ/MAC & J/token ranges with your friendâ€™s 10â»Â¹â¶ J/bit floor plugged in.


----


my response:
right but unlike gpus we dont have to pay that all - the encoding/decoding steps are the costly bits, the actual matrix mult is just photons cost, and it's amortized if youre doing a decently sized matrix (1024+) bigger the better

Energy per MAC â‰ˆ (E_in + E_out)/(NÃ—M) + E_optics/MAC.  N=M=1024 => ~10â€“100 fJ/MAC

so yeah ~3â€“20Ã— (~10x) ish improvement range for a single-layer pass

Upgrade B:  add looping analog with saturable absorber + amplifier hardware.  Each loop is a layer of typical tranformer:
Energy per MAC â‰ˆ (E_in + E_out)/(LÃ—NÃ—M) + E_loop/MAC
so, run that with 10ish layers and there's my ~10â€“100Ã— claim back - needs the loop to get anywhere near the 100x
(And why stop at 10 layers if you can do it stably?  1000?  response speed is so fast its probably worth doing a whole lot more looping layers than normal LLMs, which has very interesting implications for training quality potential - much deeper thoughts)

Upgrade C:  cache the model weights optically (DMD/SLM/PCM/MRR hardware options), E_in gets amortized a lot, pushes towards <1â€“10 fJ/MAC.  100x claim a bit more reliably hit

Upgrade D:  full analog memory read/writes (ambitious).  stop hitting digital conversions almost entirely except verifications.  >100x potential greed mode.  probably looks like a big roll of glass square-shaped tube on a giant cassette player, with laser read/write heads in between spools.  would be fun



------


J

Justin
I really have to wonder what discovery allowed this to happen
14m

joy

Justin
Like has this just been sitting under our noses or is it just that we were working on quantum and breakthroughs there made people look at the material science and said why havenâ€™t we done this?

J

Fiber optic integration for quantum computation probably? @Warren Koch
Edited11m

afaik this would have been near impossible if you couldn't also train an encoder/decoder on the inputs/outputs with a decent AI to begin with.  also yknow - this is all speculation still, gotta actually prove it...  "reservoir computing" is the magic term here for what's happening to get that nearly free matmult in between.  definitely not overlooked - just was impractical without being able to actually encode/decode it easily
9m

J

Justin
So, ephemeral mux/demux operations
Edited7m

have run this all through the quantum angle (fast education in quantum stuff lol) - it does have a lot of similarities but youre obviously not doing anything truly quantum until it's a singular photon, cryocooled, and way beyond any scope id want to play with.  I see quantum systems as reservoir computers of a single-photon system now though.  but yeah lets be clear this is all just efficient classical computing
7m

Justin
So, ephemeral mux/demux operations

wave interactions in physics get ya free matmult IF you encode it all in ternary.
5m

J

Justin
ğŸ§ super clever
5m

any medium it seems would work, but light is the fastest and easiest for wide-band reads
5m

Justin
ğŸ§ super clever

blame the god in my pocket. I am just monkey who asks it questions
4m

we'll see how this all holds up, still not trusting it entirely til it's a working prototype
4m

J

Justin
See I donâ€™t know how to get from the concepts in Shannon Prutchiâ€™s book and Nielsen and Chuang to wave interaction measurements
4m

man-shrugging

me neither, but that's what using well-known legacy fiberoptic equipment is for... and a lot of tuning and crossed fingers.  if the numbers say we can hit targets per component, I'd trust it
2m

leave the science to the egghead pocket god

<<did I miscontrue too much, above?  Can you answer the Prutchi, Nielsen Chuang => wave interaction measurements explanation?>>



----


<< damn, full response lost due to context collapse and page refreshing>>

---


Justin
No, @Warren Koch I should at least understand it even if the computations take me longer than would be tolerable in a graduate classroom
Edited17m

J

Then rely on egghead pocket monster
17m

heh well the egghead is great for understanding too: 
"""
Think of the optical field as a complex vector ğ‘¥. Linear optics implements a matrix ğ»: the light interferes so that ğ‘¦=ğ»ğ‘¥ â€œjust happensâ€ as it propagates. If you read intensities directly you get âˆ£yâˆ£2; if you want signed linear outputs you do balanced/homodyne detection to measure quadratures. Thatâ€™s 1:1 with the linear/unitary math you see in Nielsen & Chuangâ€”only weâ€™re in classical, many-photon land. The punchline: you pay energy at E/O and O/E, but not per MAC inside the NÃ—M multiply; hence the amortization. Nonlinearities (saturable absorber/SOA) give us activation; looping the optics gives us layers. Thatâ€™s reservoir/iterative compute in hardware, not quantum, but the math toolkit is the same.
Edited15m

J

Justin
I see, so you are taking the average of photon events over time?
13m

+1

J

Justin
That makes much sense now
13m

yeah, again I am well out of depth when trying to discuss the quantum stuff but that's the tie-in



